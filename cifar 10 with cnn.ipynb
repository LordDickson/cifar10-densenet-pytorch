{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import all libraries we need\n",
    "import torch as tc\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from sklearn.utils import shuffle as shuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nbimporter in c:\\programdata\\anaconda3\\lib\\site-packages (0.3.1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install nbimporter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "import densenet\n",
    "#import densenet1\n",
    "import cNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on CPU\n"
     ]
    }
   ],
   "source": [
    "def is_GPU():\n",
    "    train_mode = tc.cuda.is_available()\n",
    "    return \"Training on GPU\" if train_mode else \"Training on CPU\"\n",
    "\n",
    "def device():\n",
    "    return 'cuda' if tc.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(is_GPU())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#loading/preprocessing data\n",
    "def preprocessing(num_workers=2, validation_size=0.2):\n",
    "    #create the train_transform\n",
    "    train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                         transforms.RandomCrop(32, pad_if_needed=True),\n",
    "                                         transforms.ToTensor(),\n",
    "                                         transforms.Normalize(mean=(0.4914, 0.4822, 0.4465),\n",
    "                                                             std =  (0.2023, 0.1994, 0.2010))\n",
    "                                         ])\n",
    "    \n",
    "    #set up the test transform\n",
    "    test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                        transforms.Normalize(mean=(0.4914, 0.4822, 0.4465),\n",
    "                                                             std =  (0.2023, 0.1994, 0.2010))])\n",
    "    \n",
    "    #Get the data\n",
    "    train_set = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "    test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "    #define index for train and validation set\n",
    "    ind = shuff([x for x in range(len(train_set))])\n",
    "    split_size = np.int0(validation_size * len(train_set))\n",
    "    train_index, valid_index = ind[split_size:], ind[:split_size]\n",
    "\n",
    "    #train and validation samplers\n",
    "    train_sampler, valid_sampler = SubsetRandomSampler(train_index), SubsetRandomSampler(valid_index)\n",
    "\n",
    "    train_loader = tc.utils.data.DataLoader(train_set, batch_size=128, sampler=train_sampler, num_workers=num_workers)\n",
    "    valid_loader = tc.utils.data.DataLoader(train_set, batch_size=128, sampler=valid_sampler, \n",
    "                                               shuffle=False, num_workers=num_workers)\n",
    "    test_loader = tc.utils.data.DataLoader(test_set, batch_size=100, shuffle=False, num_workers = num_workers)\n",
    "\n",
    "    classes = ['plane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    \n",
    "    return train_loader, valid_loader, test_loader, classes\n",
    "\n",
    "train_loader, valid_loader, test_loader, classes = preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter either denseNet, or cNet\n",
    "#cNet could give an accuracy of about 85% while denseNet could give over 95%. \n",
    "#Increase the range of the epoch to say 201 when running on a GPU, this wouldnt make much difference for cNet though.\n",
    "mod_name = input('Enter the model to classify with (enter either denseNet or cNet)\\n ')\n",
    "while(mod_name != 'denseNet' and mod_name != 'cNet'):\n",
    "    mod_name = input('Enter the model to classify with (enter either denseNet or cNet)\\n')\n",
    "\n",
    "model = densenet.denseNet() if mod_name == 'denseNet' else cNet.convNet()\n",
    "\n",
    "#model = densenet.denseNet()\n",
    "#model = densenet2.densenet_cifar()\n",
    "\n",
    "model = model.to(device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sgd optimizer is much better for this very dataset\n",
    "#never use adam for this purpose is just for test purpose\n",
    "optimize = input('Enter the optimizer to use (enter sgd or adam) ')\n",
    "while(optimize != 'sgd' and optimize != 'adam'):\n",
    "    optimize = input('Enter the optimizer to use (enter sgd or adam) ')\n",
    "\n",
    "if optimize == 'sgd':\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "elif optimize == 'adam':\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_loss_min = np.Inf # set the lowest validation loss to infinity for the first step\n",
    "\n",
    "def train(epoch):\n",
    "    global valid_loss_min\n",
    "    # keep track of training and validation loss\n",
    "    train_loss, valid_loss = 0.0, 0.0\n",
    "    \n",
    "    #the training process\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        \n",
    "        data, target = data.to(device()), target.to(device()) # move tensors to GPU if CUDA is available\n",
    "        optimizer.zero_grad() # clear gradients of all optimized variables        \n",
    "        output = model.forward(data) #run a forward pass to compute the predicted output by forward passing inputs        \n",
    "        loss = criterion(output, target) #compute the batch loss        \n",
    "        loss.backward() #run backward pass to compute gradient of the loss        \n",
    "        optimizer.step() #take an optimization step        \n",
    "        train_loss += loss.item() #update the training loss\n",
    "        \n",
    "    #the valuation process\n",
    "    model.eval()\n",
    "    for data, target in valid_loader:\n",
    "        \n",
    "        data, target = data.to(device()), target.to(device()) #if CUDA exists, move tensor to GPU        \n",
    "        output = model.forward(data) #run a forward pass to compute the predicted output by forward passing inputs        \n",
    "        loss = criterion(output, target)#compute the batch loss                \n",
    "        valid_loss += loss.item() #update the validation loss\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "        \n",
    "    # print training/validation results \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch, train_loss, valid_loss))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saved ...'.format(valid_loss_min,valid_loss))\n",
    "        tc.save(model.state_dict(), 'model_saved.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "    else:\n",
    "        print('Validation loss did not decrease.   Not saved...')\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the train block\n",
    "for epoch in range(1, 31):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the best model (ie the model with the least validation loss)\n",
    "model.load_state_dict(tc.load('model_saved.pt'))\n",
    "#classes = ('plane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "#test the model\n",
    "def test():\n",
    "    global classes\n",
    "    test_loss = 0.0\n",
    "    correct = np.zeros(10)\n",
    "    total = np.zeros(10)\n",
    "    \n",
    "    model.eval() #enter evaluation mode\n",
    "    \n",
    "    #run an iteration over the test set\n",
    "    with tc.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device()), target.to(device()) #move tensor to GPU, if exists\n",
    "            output = model.forward(data) #run a forward pass\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item()\n",
    "        \n",
    "            _, pred = output.max(1) #predicted class output probabilities\n",
    "            total += target.size(0)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "            \n",
    "            return ('Overall Accuracy: %3d%%' % (100. * np.sum(correct)/np.sum(total)) )                                                   \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the test block\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#uncomment this if it says string out of range\n",
    "#classes = ['plane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "def show_image(images):\n",
    "    inverse_normalize = transforms.Normalize(mean=(-0.4914/0.2023, -0.4822/0.1994, -0.4465/0.2010),\n",
    "                                             std =  (1/0.2023, 1/0.1994, 1/0.2010))\n",
    "    \n",
    "    images = inverse_normalize(images)\n",
    "    plt.imshow(np.transpose(images, (1, 2, 0)))\n",
    "\n",
    "\n",
    "def visualize(test_loader):\n",
    "    global classes\n",
    "    images, labels = next(iter(test_loader)) #get a batch of test image\n",
    "    images.numpy()\n",
    "    images.to(device())\n",
    "\n",
    "    output = model.forward(images) #get output sample\n",
    "\n",
    "    _, pred = output.max(1) #convert output probabilities to predicted class. We can use _, pred = tc.max(output, 1)\n",
    "    out_preds = np.squeeze(pred.cpu().numpy()) if device() == 'cuda' else np.squeeze(pred.numpy())\n",
    "\n",
    "    #plot the batch images\n",
    "    fig = plt.figure(figsize=(25, 4))\n",
    "    for idx in np.arange(20):\n",
    "        ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "        #unnormalize image\n",
    "        show_image(images[idx])\n",
    "        ax.set_title(\"{} ({})\".format(classes[out_preds[idx]], classes[labels[idx]]),\n",
    "                     color=(\"blue\" if out_preds[idx]==labels[idx].item() else \"red\"))\n",
    "\n",
    "visualize(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def your_custom_image(image):\n",
    "    #classifies ['plane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']    \n",
    "    transform = transforms.Compose([\n",
    "                                    transforms.RandomResizedCrop(32),\n",
    "                                    \n",
    "                                    transforms.RandomCrop(32, pad_if_needed=True),\n",
    "                                    transforms.ToTensor(),                                    \n",
    "                                    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465),\n",
    "                                                             std =  (0.2023, 0.1994, 0.2010))])\n",
    "    my_input = transform(my_image).unsqueeze_(0)\n",
    "    my_input.numpy()\n",
    "    output = model.forward(my_input)\n",
    "    _, pred = output.max(1)\n",
    "        \n",
    "    outpred = np.squeeze(pred.numpy())\n",
    "    return classes[outpred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_image = Image.open('./img_test.jpg')\n",
    "your_custom_image(my_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
